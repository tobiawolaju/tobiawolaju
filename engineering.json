{
  "pageTitle": "Tobiloba Awolaju - Robotics Engineering Portfolio",
  "theme": "engineering",
  "profile": {
    "image": "pfp.jpg",
    "name": "Tobiloba Awolaju",
    "title": "Robotics Research Engineer | Perception, Control & Learning Systems",
    "bio": "<p style=\"font-size: large; padding-bottom: 3rem;\">Systems-level robotics engineer focused on perception, control, and learning for high-DOF robots. <br><br>I design and build end-to-end robotic systems—mechanics, control, sensing, and embedded AI—optimized for real-time performance and modularity. <br><br>Currently architecting a full-scale humanoid robot with AI-based state estimation and task execution. <br><br>I aim to develop intelligent, low-cost robotic systems that bridge research and practical applications.</p>",
    "resumeLink": "https://docs.google.com/document/d/1rAaz1npJsNaA1Z4rRAQerH271eRiXhrEhwdmj5DxFQg/edit?usp=sharing"
  },
  "sectionLabels": {
    "docs": "Technical Write-ups",
    "activity": "Git"
  },
  "filters": [
    {
      "id": "all",
      "label": "All"
    },
    {
      "id": "control",
      "label": "Control & Actuation"
    },
    {
      "id": "perception",
      "label": "Perception & World Modeling"
    },
    {
      "id": "embedded",
      "label": "Embedded Systems"
    }
  ],
  "projects": [
    {
      "category": "control",
      "image": "grey.jpg",
      "title": "Full-Body Humanoid (Control + Actuation) ↗",
      "description": "Low-cost, high-performance humanoid actuation featuring capstan-driven gimbal joints and hierarchical PID/MPC control.",
      "links": [
        {
          "text": "[3D Files]",
          "url": "#"
        },
        {
          "text": "[GitHub Repo]",
          "url": "#"
        },
        {
          "text": "[Demo Video]",
          "url": "#"
        },
        {
          "text": "[Docs]",
          "url": "#"
        }
      ],
      "writeup": {
        "motivation": "Designed a low-cost, high-DOF humanoid sleeve to test capstan-driven actuation and gimbal-jointed mechanics for scalable humanoid control.",
        "objectives": [
          "Enable 6 DOF per limb with smooth motion",
          "Maintain stability under ±10° perturbations",
          "Run on <50 W power budget",
          "Demonstrate integration with ROS2 planning stack"
        ],
        "architecture": "Hierarchical structure: ROS2 Gait Planner → PID Controller → Joint Encoders & Capstan Drives.",
        "hardware": "Capstan-driven gimbal joints for zero-backlash motion. Fusion 360 designed mechanics, optimized for weight/torque ratio.",
        "software": "Real-time PID control on firmware, integrated with MPC for gait stabilization in ROS2.",
        "perception": "Absolute magnetic encoders for state estimation; IMU integration for balance feedback.",
        "challenges": "Motor backlash compensated with high-tension capstan feedforward; custom PID tuning for low-cost actuators.",
        "results": "Measured <0.1° repeatability; successful 20-minute stability tests under perturbation.",
        "lessons": "Trade-off between mechanical complexity and control reliability; capstan drives provide superior stiffness.",
        "future": "Full-body bipedal locomotion integration and structural battery reinforcement.",
        "references": "Open-source ROS2 Control, EtherCAT protocols."
      }
    },
    {
      "category": "perception",
      "image": "grey.jpg",
      "title": "Mono-Camera World Generator / Collaborative Digital Twin ↗",
      "description": "Real-time, low-cost world generator that builds continuous 3D semantic maps from a single camera, optionally augmented with IMU/GPS, enabling multi-robot collaboration and high-level reasoning through LLM-guided task planning.",
      "links": [
        {
          "text": "[Docs]",
          "url": "#"
        },
        {
          "text": "[APK Demo]",
          "url": "#"
        },
        {
          "text": "[Engineering Log / GitHub]",
          "url": "#"
        }
      ],
      "writeup": {
        "motivation": "Bridging the gap between low-cost sensors and high-level intelligence by creating a robust, shared digital twin from mono-camera data.",
        "objectives": [
          "Generate continuous 3D semantic maps from single camera input",
          "Enable multi-robot collaboration and cloud map synchronization",
          "Provide a sparse, semantic 3D map for LLM-guided task planning",
          "Extensible for planning & digital twin applications"
        ],
        "architecture": "Mono Camera → Visual SLAM → Semantic Labeling → World Model → LLM Planner → Robot Commands.",
        "hardware": "Android device (Single Camera + IMU) or Custom ESP32-CAM nodes.",
        "software": "Custom SLAM backend, Semantic Segmentation Network, Cloud Sync Server, LLM Interface.",
        "perception": "Real-time monocular SLAM fused with semantic segmentation to build object-oriented world representations.",
        "challenges": "Drift mitigation in long-term mapping; synchronizing maps from multiple agents; real-time semantic inference on edge.",
        "results": "Successful live mapping on single device; multi-robot collaboration/cloud sync demonstrated; LLM task planning integration.",
        "lessons": "Hybridizing classical SLAM with semantic understanding enables far richer robot-environment interaction.",
        "future": "Integration with live humanoid actuation for closed-loop visual navigation in the generated world.",
        "references": "ORB-SLAM3, NeRF, LLM-based Planning research."
      }
    }
  ],
  "docs": [
    {
      "link": "#",
      "title": "Capstan-Driven Humanoid Control ↗",
      "description": "Deep dive into the mechanics and control of high-DOF humanoid joints."
    },
    {
      "link": "#",
      "title": "Mono-Camera SLAM & Collaborative World Models ↗",
      "description": "Research-level approach to real-time 3D world generation using minimal sensors, semantic fusion, and multi-robot collaborative mapping."
    },
    {
      "link": "#",
      "title": "Action Implementation & Replication via Visual Input and Natural Language ↗",
      "description": "Demonstrates how generated world models, motion planning algorithms, and LLM-guided reasoning can be combined to enable robots to interpret natural language commands, plan paths, and replicate tasks in a continuously updated 3D environment."
    }
  ]
}